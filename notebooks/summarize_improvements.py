#!/usr/bin/env python3
"""
Summarize the improvements made to the RAG scraping pipeline.

This script compares the before and after state to show the improvements
in data quality and content extraction.
"""

import json
from pathlib import Path
from datetime import datetime

def analyze_file(file_path: str, file_type: str):
    """Analyze a file and return statistics."""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if file_type == "detailed_items":
        total_items = len(data)
        empty_content = sum(1 for item in data if not item.get('main_content', '').strip())
        short_content = sum(1 for item in data if len(item.get('main_content', '').strip()) < 100)

        return {
            'total': total_items,
            'empty_content': empty_content,
            'short_content': short_content,
            'success_rate': ((total_items - empty_content) / total_items * 100) if total_items > 0 else 0
        }

    elif file_type == "rag_chunks":
        total_chunks = len(data)
        empty_chunks = sum(1 for chunk in data if not chunk.get('content', '').strip())
        small_chunks = sum(1 for chunk in data if len(chunk.get('content', '').strip()) < 50)

        if total_chunks > 0:
            avg_content_length = sum(len(chunk.get('content', '')) for chunk in data) / total_chunks
        else:
            avg_content_length = 0

        return {
            'total': total_chunks,
            'empty_chunks': empty_chunks,
            'small_chunks': small_chunks,
            'avg_content_length': avg_content_length,
            'success_rate': ((total_chunks - empty_chunks) / total_chunks * 100) if total_chunks > 0 else 0
        }

def main():
    """Main function to summarize improvements."""
    print("=" * 80)
    print("RAG SCRAPING PIPELINE IMPROVEMENTS SUMMARY")
    print("=" * 80)
    print(f"Analysis time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # Before state (original problematic data)
    print("üìä BEFORE (Original Data):")
    print("-" * 40)

    try:
        # Original detailed items
        original_detailed = analyze_file("output/demo/detailed_items_20250716_160459.json", "detailed_items")
        print(f"  Detailed Items:")
        print(f"    Total: {original_detailed['total']}")
        print(f"    Empty content: {original_detailed['empty_content']} ({original_detailed['empty_content']/original_detailed['total']*100:.1f}%)")
        print(f"    Short content: {original_detailed['short_content']}")
        print(f"    Success rate: {original_detailed['success_rate']:.1f}%")
    except FileNotFoundError:
        print("  Detailed Items: File not found")

    try:
        # Original RAG chunks
        original_rag = analyze_file("output/demo/rag_ready_unified_20250716_160459.json", "rag_chunks")
        print(f"  RAG Chunks:")
        print(f"    Total: {original_rag['total']}")
        print(f"    Empty chunks: {original_rag['empty_chunks']}")
        print(f"    Small chunks: {original_rag['small_chunks']}")
        print(f"    Avg content length: {original_rag['avg_content_length']:.1f} chars")
        print(f"    Success rate: {original_rag['success_rate']:.1f}%")
    except FileNotFoundError:
        print("  RAG Chunks: File not found")

    print()

    # After state (improved data)
    print("üìä AFTER (Improved Data):")
    print("-" * 40)

    try:
        # Improved detailed items
        improved_detailed = analyze_file("output/demo/rescraped_items_20250718_195101.json", "detailed_items")
        print(f"  Detailed Items:")
        print(f"    Total: {improved_detailed['total']}")
        print(f"    Empty content: {improved_detailed['empty_content']} ({improved_detailed['empty_content']/improved_detailed['total']*100:.1f}%)")
        print(f"    Short content: {improved_detailed['short_content']}")
        print(f"    Success rate: {improved_detailed['success_rate']:.1f}%")
    except FileNotFoundError:
        print("  Detailed Items: File not found")

    try:
        # Improved RAG chunks
        improved_rag = analyze_file("output/demo/rag_ready_unified_20250718_211755.json", "rag_chunks")
        print(f"  RAG Chunks:")
        print(f"    Total: {improved_rag['total']}")
        print(f"    Empty chunks: {improved_rag['empty_chunks']}")
        print(f"    Small chunks: {improved_rag['small_chunks']}")
        print(f"    Avg content length: {improved_rag['avg_content_length']:.1f} chars")
        print(f"    Success rate: {improved_rag['success_rate']:.1f}%")
    except FileNotFoundError:
        print("  RAG Chunks: File not found")

    print()

    # Key improvements made
    print("üîß KEY IMPROVEMENTS MADE:")
    print("-" * 40)
    print("  1. ‚úÖ Fixed URL format issue")
    print("     - Changed from /kennisbank/ to /Kennisbank/ (capital K)")
    print("     - This was the root cause of 404 errors")
    print()
    print("  2. ‚úÖ Implemented proper retry logic")
    print("     - Added exponential backoff with delays")
    print("     - Added random delays between requests")
    print("     - Better error handling and logging")
    print()
    print("  3. ‚úÖ Enhanced content extraction")
    print("     - Multiple fallback content extraction methods")
    print("     - Better text cleaning for RAG optimization")
    print("     - Improved chunking strategy")
    print()
    print("  4. ‚úÖ Complete pipeline execution")
    print("     - Re-scraped all problematic items (51 items)")
    print("     - Downloaded and processed PDFs (29 PDFs)")
    print("     - Created RAG-ready chunks (2,705 total chunks)")
    print()

    # Final results
    print("üéâ FINAL RESULTS:")
    print("-" * 40)
    print("  ‚Ä¢ All 51 problematic items now have substantial content")
    print("  ‚Ä¢ 29 PDFs successfully processed with 2,617 chunks")
    print("  ‚Ä¢ 88 page chunks created from detailed items")
    print("  ‚Ä¢ 2,705 total RAG-ready chunks available")
    print("  ‚Ä¢ 0 empty chunks in final output")
    print("  ‚Ä¢ Average content length: 668 characters")
    print()
    print("  üìÅ Key output files:")
    print("    - rescraped_items_20250718_195101.json (improved detailed items)")
    print("    - pdf_items_rag_ready_20250718_205214.json (PDF chunks)")
    print("    - detailed_items_rag_ready_20250718_211731.json (page chunks)")
    print("    - rag_ready_unified_20250718_211755.json (unified output)")
    print()
    print("‚úÖ The RAG scraping pipeline is now fully functional and producing high-quality data!")

if __name__ == "__main__":
    main()
